# ğŸš— Car Sales End-to-End Data Engineering Project on Azure

This project demonstrates a full-scale **end-to-end data engineering pipeline** implemented on **Microsoft Azure**, tailored for handling **semi-structured car sales data** from ingestion to analytics-ready star schema format. It leverages modern data engineering practices such as the **Medallion Architecture**, **Delta Lake**, **Azure Data Factory**, and **Azure Databricks**, making it job-ready and aligned with real-world enterprise use cases.

---

## ğŸ“¸ Pipeline Architecture

![Pipeline Architecture](End_to_End_pipeline.jpg)

*End-to-End Azure Data Engineering Pipeline â€” built with ADF, ADLS Gen2, Databricks, Delta Lake*

---

## ğŸš€ Tech Stack

- **Azure Data Factory** â€“ Data ingestion & orchestration  
- **Azure Data Lake Gen2** â€“ Storage in Bronze, Silver, and Gold zones  
- **Azure Databricks** â€“ Data transformation using PySpark  
- **Delta Lake** â€“ ACID-compliant analytical storage  
- **Azure SQL / Synapse** â€“ Source system (simulated)  
- **GitHub** â€“ Source code versioning  
- **Parameterized datasets & dynamic pipelines** â€“ Scalable and reusable ETL  

---

## ğŸ—ï¸ Architecture: Medallion Layer

| Layer      | Description |
|------------|-------------|
| **Bronze** | Raw incremental data in Parquet from SQL source |
| **Silver** | Cleaned and transformed data for dimensional modeling |
| **Gold**   | Fact and dimension tables in star schema using Delta Lake |

---

## ğŸ“‚ Pipeline Flow

1. **Ingestion (Bronze)**
   - Lookup last watermark & current timestamp
   - Copy only new/updated records from SQL to ADLS in Parquet format
   - Update watermark using stored procedure

2. **Transformation (Silver)**
   - Clean, join, enrich data using PySpark notebooks in Databricks
   - Generate a unified "one big table"

3. **Serving (Gold)**
   - Build dimension tables: `dim_branch`, `dim_dealer`, `dim_model`, `dim_date`
   - Build fact table: `factsales` using surrogate keys and slowly changing dimensions (SCD)
   - All final outputs stored in Delta format

---

## ğŸ“Š Business Use Case

This pipeline simulates a real-time **car dealership sales system**, where:
- New sales transactions arrive daily  
- Business needs include dealer performance, car model trends, branch-wise comparison  
- The goal is to provide an **analytics-ready schema** (Star Schema) for BI tools like Power BI/Tableau  

---

## ğŸ› ï¸ Features

- âœ… Incremental Loading via Parameterized Watermarks  
- âœ… Dynamic Data Factory Pipelines using Parameters  
- âœ… Modular Notebooks per Dimension & Fact  
- âœ… Schema Evolution with Delta Lake  
- âœ… Job orchestration with ADF pipeline + notebooks  
- âœ… Version control via GitHub  

---

## ğŸ§  Concepts Practiced

- ETL / ELT workflow orchestration  
- Azure Data Factory parameterized datasets  
- Delta Lake time travel and schema enforcement  
- Star schema design and surrogate key handling  
- Slowly Changing Dimensions (SCD Type 1 & 2)  

---

